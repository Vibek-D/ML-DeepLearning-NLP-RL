{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are using an Autoencoder to encode the digits 1 and 2 and we indivisually make the classes for the encoder and the decoder and then we concatenate both the functions later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "import numpy as np \n",
    "from tensorflow import set_random_seed\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 2]\n",
      " [1 1 2]\n",
      " [1 2 1]\n",
      " ...\n",
      " [1 1 2]\n",
      " [1 1 2]\n",
      " [2 1 1]]\n",
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 0s 328us/step - loss: 1.7200\n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.4854\n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.3288\n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.2914\n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2751\n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2634\n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.2538\n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.248 - 0s 47us/step - loss: 0.2455\n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.2385\n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2323\n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2267\n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2219\n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.2177\n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.2138\n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2103\n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.2070\n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.2040\n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.2014\n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 0s 109us/step - loss: 0.1990\n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 0.1967\n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 0s 125us/step - loss: 0.1946\n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 0s 141us/step - loss: 0.1928\n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1909\n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.1893\n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1877\n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1863\n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.184 - 0s 78us/step - loss: 0.1848\n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.1835\n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1822\n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1811\n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.1799\n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.1789\n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1781\n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1770\n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1761\n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1753\n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1744\n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1737\n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1729\n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1722\n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1677\n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1621\n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1584\n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.1549\n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1519\n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1491\n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1466\n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1445\n",
      "Epoch 49/300\n",
      "1000/1000 [==============================] - 0s 156us/step - loss: 0.1420\n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1389\n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1352\n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.1319\n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1289\n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1261\n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1236\n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.1213\n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1190\n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 0.1170\n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1153\n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1135\n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 0.1118\n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1103\n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.1088\n"
     ]
    }
   ],
   "source": [
    "def seedy(s):\n",
    "    np.random.seed(s)\n",
    "    set_random_seed(s)\n",
    "\n",
    "class AutoEncoder:\n",
    "    def __init__(self, encoding_dim=3):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        r = lambda: np.random.randint(1, 3)\n",
    "        self.x = np.array([[r(),r(),r()] for _ in range(1000)])\n",
    "        print(self.x)\n",
    "\n",
    "    def _encoder(self):\n",
    "        inputs = Input(shape=(self.x[0].shape))\n",
    "        encoded = Dense(self.encoding_dim, activation='relu')(inputs)\n",
    "        model = Model(inputs, encoded)\n",
    "        self.encoder = model\n",
    "        return model\n",
    "\n",
    "    def _decoder(self):\n",
    "        inputs = Input(shape=(self.encoding_dim,))\n",
    "        decoded = Dense(3)(inputs)\n",
    "        model = Model(inputs, decoded)\n",
    "        self.decoder = model\n",
    "        return model\n",
    "\n",
    "    def encoder_decoder(self):\n",
    "        ec = self._encoder()\n",
    "        dc = self._decoder()\n",
    "        \n",
    "        inputs = Input(shape=self.x[0].shape)\n",
    "        ec_out = ec(inputs)\n",
    "        dc_out = dc(ec_out)\n",
    "        model = Model(inputs, dc_out)\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def fit(self, batch_size=10, epochs=300):\n",
    "        self.model.compile(optimizer='sgd', loss='mse')\n",
    "        log_dir = './log/'\n",
    "        tbCallBack = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True)\n",
    "        self.model.fit(self.x, self.x,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[tbCallBack])\n",
    "\n",
    "    def save(self):\n",
    "        if not os.path.exists(r'./weights'):\n",
    "            os.mkdir(r'./weights')\n",
    "        else:\n",
    "            self.encoder.save(r'./weights/encoder_weights.h5')\n",
    "            self.decoder.save(r'./weights/decoder_weights.h5')\n",
    "            self.model.save(r'./weights/ae_weights.h5')\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seedy(2)\n",
    "    ae = AutoEncoder(encoding_dim=2)\n",
    "    ae.encoder_decoder()\n",
    "    ae.fit(batch_size=50, epochs=300)\n",
    "    ae.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the model.fit(x,x), ie the inputs and the outputs are the same in an Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[1 7 3]]\n",
      "Encoded: [[0.       4.185444]]\n",
      "Decoded: [[3.5106544 3.6673677 3.56367  ]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "encoder = load_model(r'./weights/encoder_weights.h5')\n",
    "decoder = load_model(r'./weights/decoder_weights.h5')\n",
    "\n",
    "inputs = np.array([[1,7,3]])\n",
    "x = encoder.predict(inputs)\n",
    "y = decoder.predict(x)\n",
    "\n",
    "print('Input: {}'.format(inputs))\n",
    "print('Encoded: {}'.format(x))\n",
    "print('Decoded: {}'.format(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
